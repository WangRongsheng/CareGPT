> **Note**
>
> åœ¨çº¿ä½“éªŒğŸ§‘â€ğŸš€CareLlama/CareGPTï¼šhttps://huggingface.co/spaces/wangrongsheng/CareLlama

> **warning**
> 
> **CareLlama/CareGPTä¸ºMPUçš„åŒ»ç–—å¤§è¯­è¨€æ¨¡å‹IvyGPTçš„åˆ†æ”¯ï¼Œå…¶å­˜åœ¨æ„ä¹‰æ˜¯æ¢ç´¢åŒ»ç–—æ•°æ®ã€åŒ»ç–—LLMè®­ç»ƒä¸éƒ¨ç½²ç›¸å…³çš„å·¥ä½œç ”ç©¶ã€‚**

<div align="center">
  <a href="https://github.com/WangRongsheng/ChatGenTitle">
    <img src="https://github.com/WangRongsheng/CareLlama/blob/main/assets/images/home.png" alt="Logo" height="280">
  </a>

  <p align="center">
    <h3> CareLlama/CareGPT (å…³æ€€ç¾Šé©¼)ï¼šåŒ»ç–—LLMï¼Œå¼€æºé©±åŠ¨ï¼Œå…±åˆ›å¥åº·æœªæ¥ </h3>
    <p align="center">
      <em>èµ„æºæ•´åˆ / å¼€æºæ¨¡å‹ / ä¸°å¯Œæ•°æ® / é«˜æ•ˆéƒ¨ç½² / LLaMA</em>
    </p>
    <p align="center">
      <a href='https://github.com/WangRongsheng/CareLlama'>
            <img src='https://img.shields.io/badge/Project-Page-Green'>
      </a>
      <a href='https://github.com/WangRongsheng/CareLlama'>
            <img src='https://img.shields.io/badge/Paper-Arxiv-red'>
      </a>
      <a href="#">
        <img alt="GitHub Contributors" src="https://colab.research.google.com/assets/colab-badge.svg" />
      </a>
      <a href='https://huggingface.co/wangrongsheng'>
        <img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue'>
      </a>
      </br>
      <a href="https://github.com/WangRongsheng/CareLlama/graphs/contributors">
        <img alt="GitHub Contributors" src="https://img.shields.io/github/contributors/WangRongsheng/CareLlama" />
      </a>
      <a href="https://github.com/WangRongsheng/CareLlama/issues">
        <img alt="Issues" src="https://img.shields.io/github/issues/WangRongsheng/CareLlama?color=0088ff" />
      </a>
      <a href="https://github.com/WangRongsheng/CareLlama/pulls">
        <img alt="GitHub pull requests" src="https://img.shields.io/github/issues-pr/WangRongsheng/CareLlama?color=0088ff" />
      </a>
      <a href=href="https://github.com/WangRongsheng/CareLlama/stargazers">
        <img src="https://img.shields.io/github/stars/WangRongsheng/CareLlama?color=ccf">
      </a>
      <a href=href="https://github.com/WangRongsheng/CareLlama">
        <img src="https://img.shields.io/github/repo-size/WangRongsheng/CareLlama.svg?style=flat-square">
      </a>
      </br>
      <a href=href="https://github.com/WangRongsheng/CareLlama">
        <img src="https://visitor-badge.laobi.icu/badge?page_id=https://github.com/WangRongsheng/CareLlama">
      </a>
      <a href=href="https://github.com/WangRongsheng/CareLlama">
        <img src="https://img.shields.io/github/last-commit/WangRongsheng/CareLlama">
      </a>
      <a href="https://github.com/WangRongsheng/CareLlama/blob/main/LICENSE">
        <img alt="GitHub Contributors" src="https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg" />
      </a>
  </p>
</div>

<!--center><kbd><img src="./docs/images/usage.png" height="550px"/></kbd></center-->

<p align="center">
      <a href="#"><strong>è§†é¢‘æ•™ç¨‹</strong></a>
      <a href="https://github.com/WangRongsheng/CareLlama/tree/main#5gradio%E9%83%A8%E7%BD%B2"><strong>å®‰è£…éƒ¨ç½²</strong></a>
      <a href="https://huggingface.co/spaces/wangrongsheng/CareLlama"><strong>åœ¨çº¿ä½“éªŒ</strong></a>
</p>

![](./assets/images/hx.png)

âš¡ç‰¹æ€§ï¼š
1. æ·»åŠ [ChatGPT fine-tuning](https://github.com/WangRongsheng/CareLlama/tree/main/ChatGPT)å®ç°ï¼Œæ¨èæœ‰é¢åº¦çš„æœ‹å‹åœ¨ChatGPTä¸Šè¿›è¡Œå¾®è°ƒå®éªŒï¼›
2. æ”¯æŒ[ChatGPT-Next-Web](https://github.com/WangRongsheng/CareLlama/tree/main#6chatgpt-next-web%E9%83%A8%E7%BD%B2)éƒ¨ç½²å¾®è°ƒçš„æ¨¡å‹ï¼›
3. æ”¯æŒ[Gradio](https://github.com/WangRongsheng/CareLlama/tree/main#5gradio%E9%83%A8%E7%BD%B2)éƒ¨ç½²å¾®è°ƒçš„æ¨¡å‹ï¼›
4. æ”¯æŒLLaMAã€LLaMA-2å…¨ç³»åˆ—æ¨¡å‹è®­ç»ƒï¼›
5. æ”¯æŒLoRAã€QLoRAï¼ŒåŒ…æ‹¬åç»­PPOã€DPOå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼›
6. æ”¯æŒæ¨¡å‹ä¸çŸ¥è¯†åº“ç»“åˆé—®ç­”ï¼›
7. å¼€æºäº†è¶…è¿‡[60ä¸ªåŒ»é™¢ç§‘å®¤çš„å¯¼è¯Šææ–™ä¿¡æ¯](https://github.com/WangRongsheng/CareLlama/tree/main/data/Hospital%20Guide)ï¼›
8. å¼€å‘äº†æ”¯æŒ[GPT-4/ChatGPTæ¨¡å‹è’¸é¦åŒ»å­¦æ•°æ®](https://huggingface.co/spaces/wangrongsheng/DataMaker)çš„å·¥å…·ï¼Œèƒ½å¤Ÿæ‰¹é‡ç”Ÿæˆå„ç§ç”¨äºæ„å»ºçŸ¥è¯†åº“å’Œå¾®è°ƒçš„æ•°æ®ï¼›
9. èšåˆäº†ä¸°å¯Œçš„å¼€æºåŒ»å­¦LLMã€LLMè®­ç»ƒçš„åŒ»å­¦æ•°æ®ã€LLMéƒ¨ç½²èµ„æ–™ã€LLMæµ‹è¯„ä»¥åŠç›¸å…³LLMçš„èµ„æºæ•´ç†ï¼›
10. æˆ‘ä»¬å‚ä¸äº†åŒ»å­¦LLMçš„[CMBæ¦œå•è¯„æµ‹-IvyGPT](https://cmedbenchmark.llmzoo.com/static/leaderboard.html)ï¼Œåœ¨æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬é¢†å…ˆChatGPTåŠä¸€ä¼—å¼€æºåŒ»å­¦LLMï¼›
11. æˆ‘ä»¬åŸºäºè‡ªæœ‰æ•°æ®é›†åœ¨ä¸åŒåŸºåº§LLMä¸Šè®­ç»ƒå¼€æºäº†å¤šä¸ªåŒ»ç–—LLMï¼Œæ‚¨å¯ä»¥ç›´æ¥ä¸‹è½½ä½“éªŒï¼›

# ğŸæ•°æ®é›†

#### é¢„è®­ç»ƒæ•°æ®

- [LLM-Pretrain-FineTune/data_pretrain](https://github.com/X-jun-0130/LLM-Pretrain-FineTune/tree/main/data_pretrain)
- [MedicalGPT/pretrain](https://github.com/shibing624/MedicalGPT/tree/main/data/pretrain)
- [zysj](https://www.zysj.com.cn/)

#### ç›‘ç£è®­ç»ƒæ•°æ®
- [icliniq-10k(en)](https://drive.google.com/file/d/1ZKbqgYqWc7DJHs3N9TQYQVPdDQmZaClA/view?usp=sharing)
- [HealthCareMagic-100k(en)](https://drive.google.com/file/d/1lyfqIwlLSClhgrCutWuEe_IACNq6XNUt/view?usp=sharing)
- [ShenNong_TCM_Dataset](https://huggingface.co/datasets/michaelwzhu/ShenNong_TCM_Dataset)
- âœ…[ChatMed_Consult_Dataset](https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset)
- [Chinese-medical-dialogue-data](https://github.com/Toyhom/Chinese-medical-dialogue-data)
- [cMedQA2](https://github.com/zhangsheng93/cMedQA2)
- âœ…[Huatuo-26M](https://github.com/FreedomIntelligence/Huatuo-26M)
- [cMedQA2](https://github.com/zhangsheng93/cMedQA2)
- [webMedQA](https://github.com/hejunqing/webMedQA)
- [PubMedQA](https://pubmedqa.github.io/)
- [CMCQA](https://github.com/WENGSYX/CMCQA)
- âœ…[QiZhenGPT](https://github.com/CMKRG/QiZhenGPT/tree/main/data)
- âœ…[LLM-Pretrain-FineTune/data_sft](https://github.com/X-jun-0130/LLM-Pretrain-FineTune/tree/main/data_sft)
- [Medical-Dialogue-System](https://github.com/UCSD-AI4H/Medical-Dialogue-System)
- [IMCS-V2](https://github.com/lemuria-wchen/imcs21)
- [CHIP-MDCFNPC](https://tianchi.aliyun.com/dataset/95414)
- [MedDG](https://tianchi.aliyun.com/dataset/95414)
- âœ…[HuatuoGPT-sft-data-v1](https://huggingface.co/datasets/FreedomIntelligence/HuatuoGPT-sft-data-v1)
- [MedicalGPT/finetune](https://github.com/shibing624/MedicalGPT/tree/main/data/finetune)
- âœ…[shibing624/medical](https://huggingface.co/datasets/shibing624/medical)
- [medAlpaca/data](https://github.com/kbressem/medAlpaca#data-overview)
- âœ…[Zhongjing/sft](https://github.com/SupritYoung/Zhongjing)
- [medical_dialog](https://huggingface.co/datasets/medical_dialog)
- [huatuo_encyclopedia_qa](https://huggingface.co/datasets/FreedomIntelligence/huatuo_encyclopedia_qa)
- [Med-ChatGLM/data](https://github.com/SCIR-HI/Med-ChatGLM/tree/main/data)
- [CMB](https://github.com/FreedomIntelligence/CMB)
- [GenMedGPT-5k(en)](https://drive.google.com/file/d/1nDTKZ3wZbZWTkFMBkxlamrzbNz0frugg/view?usp=sharing)
- [Alpaca-CoT(general)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)
- âœ…[DISC-Med-SFT](https://huggingface.co/datasets/Flmc/DISC-Med-SFT)

#### å¥–åŠ±è®­ç»ƒæ•°æ®

- [MedicalGPT/reward](https://github.com/shibing624/MedicalGPT/tree/main/data/reward)
- [Zhongjing/rw](https://github.com/SupritYoung/Zhongjing/tree/main/data)
- [comparison_gpt4_data](https://huggingface.co/datasets/wangrongsheng/comparison_gpt4_data)
- [HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf)

# ğŸ—œï¸å…¨æµç¨‹è®­ç»ƒ

## 1.å®‰è£…ä¾èµ–

```python
conda create -n llm python=3.11
conda activate llm
python -m pip install -r requirements.txt
```

- LLaMAæ¨¡å‹ä¸‹è½½ï¼šhttps://blog.csdn.net/u014297502/article/details/129829677
```python
# è½¬ä¸ºHFæ ¼å¼
python -m transformers.models.llama.convert_llama_weights_to_hf \
    --input_dir path_to_llama_weights --model_size 7B --output_dir path_to_llama_model
```
- LLaMA-2æ¨¡å‹ä¸‹è½½ï¼šhttps://huggingface.co/meta-llama

## 2.æ•°æ®é…ç½®

<details>
<summary>æ•°æ®é›†é…ç½®ã€PTã€SFTã€RWæ•°æ®æ ¼å¼</summary>

### dataset_info

å¦‚æœæ‚¨ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†ï¼Œè¯·åŠ¡å¿…åœ¨ `dataset_info.json` æ–‡ä»¶ä¸­ä»¥å¦‚ä¸‹æ ¼å¼æä¾›æ‚¨çš„æ•°æ®é›†å®šä¹‰ã€‚

```json
"æ•°æ®é›†åç§°": {
  "hf_hub_url": "HuggingFaceä¸Šçš„é¡¹ç›®åœ°å€ï¼ˆè‹¥æŒ‡å®šï¼Œåˆ™å¿½ç•¥ä¸‹åˆ—ä¸‰ä¸ªå‚æ•°ï¼‰",
  "script_url": "åŒ…å«æ•°æ®åŠ è½½è„šæœ¬çš„æœ¬åœ°æ–‡ä»¶å¤¹åç§°ï¼ˆè‹¥æŒ‡å®šï¼Œåˆ™å¿½ç•¥ä¸‹åˆ—ä¸¤ä¸ªå‚æ•°ï¼‰",
  "file_name": "è¯¥ç›®å½•ä¸‹æ•°æ®é›†æ–‡ä»¶çš„åç§°ï¼ˆè‹¥ä¸Šè¿°å‚æ•°æœªæŒ‡å®šï¼Œåˆ™æ­¤é¡¹å¿…éœ€ï¼‰",
  "file_sha1": "æ•°æ®é›†æ–‡ä»¶çš„SHA-1å“ˆå¸Œå€¼ï¼ˆå¯é€‰ï¼‰",
  "columns": {
    "prompt": "æ•°æ®é›†ä»£è¡¨æç¤ºè¯çš„è¡¨å¤´åç§°ï¼ˆé»˜è®¤ï¼šinstructionï¼‰",
    "query": "æ•°æ®é›†ä»£è¡¨è¯·æ±‚çš„è¡¨å¤´åç§°ï¼ˆé»˜è®¤ï¼šinputï¼‰",
    "response": "æ•°æ®é›†ä»£è¡¨å›ç­”çš„è¡¨å¤´åç§°ï¼ˆé»˜è®¤ï¼šoutputï¼‰",
    "history": "æ•°æ®é›†ä»£è¡¨å†å²å¯¹è¯çš„è¡¨å¤´åç§°ï¼ˆé»˜è®¤ï¼šNoneï¼‰"
  }
}
```

å…¶ä¸­ `prompt` å’Œ `response` åˆ—åº”å½“æ˜¯éç©ºçš„å­—ç¬¦ä¸²ã€‚`query` åˆ—çš„å†…å®¹å°†ä¼šå’Œ `prompt` åˆ—æ‹¼æ¥ä½œä¸ºæ¨¡å‹è¾“å…¥ã€‚`history` åˆ—åº”å½“æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²äºŒå…ƒç»„ï¼Œåˆ†åˆ«ä»£è¡¨ç”¨æˆ·è¯·æ±‚å’Œæ¨¡å‹ç­”å¤ã€‚

### PT example data

`.txt`æ ¼å¼ï¼Œä¸€è¡Œä¸€ä¸ªæ— ç›‘ç£æ•°æ®ã€‚

```html
Machine learning (ML) is a field devoted to understanding and building methods that let machines "learn" â€“ that is, methods that leverage data to improve computer performance on some set of tasks.
Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, agriculture, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.
```

### SFT example data 1

```json
[
  {
    "instruction": "å¬èµ·æ¥å¾ˆä¸é”™ã€‚äººå·¥æ™ºèƒ½å¯èƒ½åœ¨å“ªäº›æ–¹é¢é¢ä¸´æŒ‘æˆ˜å‘¢ï¼Ÿ",
    "input": "",
    "output": "äººå·¥æ™ºèƒ½é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬æ•°æ®éšç§ã€å®‰å…¨å’Œé“å¾·æ–¹é¢çš„é—®é¢˜ï¼Œä»¥åŠå½±å“å°±ä¸šæœºä¼šçš„è‡ªåŠ¨åŒ–ç­‰é—®é¢˜ã€‚",
    "history": [
      ["ä½ å¥½ï¼Œä½ èƒ½å¸®æˆ‘è§£ç­”ä¸€ä¸ªé—®é¢˜å—ï¼Ÿ", "å½“ç„¶ï¼Œè¯·é—®æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ"],
      ["æˆ‘æƒ³äº†è§£äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘ï¼Œä½ æœ‰ä»€ä¹ˆæƒ³æ³•å—ï¼Ÿ", "äººå·¥æ™ºèƒ½åœ¨æœªæ¥çš„å‘å±•æ–¹å‘å¯èƒ½åŒ…æ‹¬æ›´å¼ºå¤§çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ›´å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œä»¥åŠæ›´åŠ æ™ºèƒ½çš„æœºå™¨äººã€‚"]
    ]
  }
]
```

### SFT example data 2

```json
[
  {
    "instruction": "å¬èµ·æ¥å¾ˆä¸é”™ã€‚äººå·¥æ™ºèƒ½å¯èƒ½åœ¨å“ªäº›æ–¹é¢é¢ä¸´æŒ‘æˆ˜å‘¢ï¼Ÿ",
    "input": "",
    "output": "äººå·¥æ™ºèƒ½é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬æ•°æ®éšç§ã€å®‰å…¨å’Œé“å¾·æ–¹é¢çš„é—®é¢˜ï¼Œä»¥åŠå½±å“å°±ä¸šæœºä¼šçš„è‡ªåŠ¨åŒ–ç­‰é—®é¢˜ã€‚",
    "history": []
  }
]
```

### RW example data

```json
[
  {
    "instruction": "ç”Ÿæˆä¸‰ä¸ªä¸â€œé“æ­‰â€æ„æ€ç›¸åŒçš„åŠ¨è¯",
    "input": "",
    "output": [
      "æ‰¿è®¤ï¼Œè¡¨ç¤ºé—æ†¾ï¼Œå¼¥è¡¥ã€‚",
      "é“æ­‰"
    ]
  }
]
```
  
</details>

## 3.è®­ç»ƒé…ç½®

<details>
<summary>è®­ç»ƒå‚æ•°ä¸æŒ‡ä»¤</summary>

### é…ç½®åˆ†å¸ƒå¼

```python
accelerate config # configure the environment
accelerate launch src/train_bash.py # arguments (same as above)
```

### ç›‘ç£è®­ç»ƒ

```python
# LLaMA-2
accelerate launch src/train_bash.py \
    --stage sft \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --do_train \
    --dataset mm \
    --finetuning_type lora \
    --quantization_bit 4 \
    --overwrite_cache \
    --output_dir output \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 5e-5 \
    --num_train_epochs 2.0 \
    --plot_loss \
    --fp16 \
    --template llama2 \
    --lora_target q_proj,v_proj

# LLaMA
accelerate launch src/train_bash.py \
    --stage sft \
    --model_name_or_path ./Llama-7b-hf \
    --do_train \
    --dataset mm,hm \
    --finetuning_type lora \
    --overwrite_cache \
    --output_dir output-1 \
    --per_device_train_batch_size 4 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 2000 \
    --learning_rate 5e-5 \
    --num_train_epochs 2.0 \
    --plot_loss \
    --fp16 \
    --template default \
    --lora_target q_proj,v_proj
```

### å¼ºåŒ–å­¦ä¹ 

```python
# LLaMA-2, DPO
accelerate launch src/train_bash.py \
    --stage dpo \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --do_train \
    --dataset rlhf \
    --template llama2 \
    --finetuning_type lora \
    --quantization_bit 4 \
    --lora_target q_proj,v_proj \
    --resume_lora_training False \
    --checkpoint_dir ./output-2 \
    --output_dir output-dpo \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 1e-5 \
    --num_train_epochs 1.0 \
    --plot_loss \
    --fp16
```
  
</details>

## 4.æ¨ç†é…ç½®

<details>
<summary>æ¨ç†å‚æ•°ä¸æŒ‡ä»¤</summary>

### Webè®¿é—®

```python
# LLaMA-2
python src/web_demo.py \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --checkpoint_dir output \
    --finetuning_type lora \
    --template llama2

# LLaMA
python src/web_demo.py \
    --model_name_or_path ./Llama-7b-hf \
    --checkpoint_dir output-1 \
    --finetuning_type lora \
    --template default

# DPO
python src/web_demo.py \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --checkpoint_dir output-dpo \
    --finetuning_type lora \
    --template llama2
```

### APIè®¿é—®

```python
# LLaMA-2
python src/api_demo.py \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --checkpoint_dir output \
    --finetuning_type lora \
    --template llama2

# LLaMA
python src/api_demo.py \
    --model_name_or_path ./Llama-7b-hf \
    --checkpoint_dir output-1 \
    --finetuning_type lora \
    --template default

# DPO
python src/api_demo.py \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --checkpoint_dir output-dpo \
    --finetuning_type lora \
    --template llama2
```

æµ‹è¯•APIï¼š
```python
curl -X 'POST' \
    'http://127.0.0.1:8888/v1/chat/completions' \
    -H 'accept: application/json' \
    -H 'Content-Type: application/json' \
    -d '{
    "model": "string",
    "messages": [
      {
        "role": "user",
        "content": "ä½ å¥½"
      }
    ],
    "temperature": 0,
    "top_p": 0,
    "max_new_tokens": 0,
    "stream": false
  }'
```

### CLIè®¿é—®

```python
# LLaMA-2
python src/cli_demo.py \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --checkpoint_dir output \
    --finetuning_type lora \
    --template llama2

# LLaMA
python src/cli_demo.py \
    --model_name_or_path ./Llama-7b-hf \
    --checkpoint_dir output-1 \
    --finetuning_type lora \
    --template default

# DPO
python src/cli_demo.py \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --checkpoint_dir output-dpo \
    --finetuning_type lora \
    --template llama2
```

### æ‰¹é‡é¢„æµ‹

```python
# LLaMA-2
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
    --stage sft \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --do_predict \
    --dataset mm \
    --template llama2 \
    --finetuning_type lora \
    --checkpoint_dir output \
    --output_dir predict_output \
    --per_device_eval_batch_size 8 \
    --max_samples 100 \
    --predict_with_generate

# LLaMA
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
    --stage sft \
    --model_name_or_path ./Llama-7b-hf \
    --do_predict \
    --dataset mm \
    --template default \
    --finetuning_type lora \
    --checkpoint_dir output-1 \
    --output_dir predict_output \
    --per_device_eval_batch_size 8 \
    --max_samples 100 \
    --predict_with_generate
```

### å®éªŒè¯„ä¼°(BLEUå’ŒROUGE_CHINESE)

```python
# LLaMA-2
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
    --stage sft \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --do_eval \
    --dataset mm \
    --template llama2 \
    --finetuning_type lora \
    --checkpoint_dir output \
    --output_dir eval_output \
    --per_device_eval_batch_size 8 \
    --max_samples 100 \
    --predict_with_generate

# LLaMA
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
    --stage sft \
    --model_name_or_path ./Llama-7b-hf \
    --do_eval \
    --dataset mm \
    --template default \
    --finetuning_type lora \
    --checkpoint_dir output-1 \
    --output_dir eval_output \
    --per_device_eval_batch_size 8 \
    --max_samples 100 \
    --predict_with_generate
```

åœ¨4/8-bitè¯„ä¼°æ—¶ï¼Œæ¨èä½¿ç”¨`--per_device_eval_batch_size=1`å’Œ`--max_target_length 128`

</details>

## 5.Gradioéƒ¨ç½²

<details>
<summary>Gradioéƒ¨ç½²æŒ‡ä»¤</summary>

### æ¨¡å‹å¯¼å‡º

```python
# LLaMA-2
python src/export_model.py \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --template llama2 \
    --finetuning_type lora \
    --checkpoint_dir output-1 \
    --output_dir output_export

# LLaMA
python src/export_model.py \
    --model_name_or_path ./Llama-7b-hf \
    --template default \
    --finetuning_type lora \
    --checkpoint_dir output \
    --output_dir output_export
```

### å¼€å¯è¿è¡Œ

```python
%cd Gradio
python app.py
```

</details>

![](./Gradio/gradio-demo.png)

## 6.[ChatGPT-Next-Web](https://github.com/Yidadaa/ChatGPT-Next-Web)éƒ¨ç½²

<details>
<summary>Nextéƒ¨ç½²æŒ‡ä»¤</summary>

### å¼€å¯APIæœåŠ¡

```python
# LLaMA-2
python src/api_demo.py \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --checkpoint_dir output \
    --finetuning_type lora \
    --template llama2

# LLaMA
python src/api_demo.py \
    --model_name_or_path ./Llama-7b-hf \
    --checkpoint_dir output-1 \
    --finetuning_type lora \
    --template default
```

### ä¸‹è½½Nextå¹¶è¿è¡Œ

1. ä¸‹è½½Nextï¼š
[![Web][Web-image]][web-url]
[![Windows][Windows-image]][download-url]
[![MacOS][MacOS-image]][download-url]
[![Linux][Linux-image]][download-url]

[web-url]: https://chatgpt.nextweb.fun
[download-url]: https://github.com/Yidadaa/ChatGPT-Next-Web/releases
[Web-image]: https://img.shields.io/badge/Web-PWA-orange?logo=microsoftedge
[Windows-image]: https://img.shields.io/badge/-Windows-blue?logo=windows
[MacOS-image]: https://img.shields.io/badge/-MacOS-black?logo=apple
[Linux-image]: https://img.shields.io/badge/-Linux-333?logo=ubuntu

2. ä¿®æ”¹é…ç½®ï¼š
å®‰è£…å¹¶æ‰“å¼€Nextï¼Œç„¶åæ‰“å¼€`è®¾ç½®`ï¼Œä¿®æ”¹`æ¥å£åœ°å€`ä¸ºï¼š`http://127.0.0.1:8000/`ï¼ˆå³ä½ çš„APIæ¥å£åœ°å€ï¼‰ï¼Œç„¶åå°±å¯ä»¥ä½¿ç”¨äº†ã€‚

</details>

![](./Next/chatgpt-next-web.png)

# ğŸ’«å®è·µç»éªŒ

1. åœ¨CareLlamaä¸­å¹¶æœªå¯¹åˆ†è¯æ¨¡å‹è¿›è¡Œä¸­æ–‡åˆ†è¯çš„æ·»åŠ å’Œé‡æ–°è®­ç»ƒï¼Œä½†æ˜¯æ•ˆæœä¾æ—§è¡¨ç°å¯å–œï¼›
2. å…¨æµç¨‹çš„LLMè®­ç»ƒåŒ…æ‹¬ï¼šé¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒã€å¥–åŠ±æ¨¡å‹ã€å¼ºåŒ–å­¦ä¹ ï¼Œ**å¤šæ•°æƒ…å†µä¸‹ç›‘ç£å¾®è°ƒå³å¯æ»¡è¶³è‡ªèº«éœ€æ±‚**ï¼›
3. åœ¨ç®—åŠ›å……è¶³æƒ…å†µä¸‹æ¨è**ä½¿ç”¨åŒ»ç–—æ•°æ®å’Œé€šç”¨è¯­æ–™æ•°æ®è¿›è¡Œè®­ç»ƒ**ï¼Œè¿™æ ·æ¨¡å‹æ—¢å¯ä»¥æœ‰åŒ»å­¦ä¸Šçš„è®­ç»ƒå­¦ä¹ ï¼Œä¹Ÿå¯ä»¥ä¿æŒé€šç”¨èƒ½åŠ›ï¼ˆå¦‚æŒ‡ä»¤éµå¾ªï¼‰ï¼›
4. ä¸è¦æŒ‡æœ›ä¸€ä¸ªåŒ»ç–—LLMå°±å¯ä»¥æ»¡è¶³æ‰€æœ‰éœ€æ±‚ï¼Œåˆç†çš„åšæ³•å¯èƒ½æ˜¯å®æ—¶æ›´æ–°çš„**çŸ¥è¯†åº“+å¾®è°ƒçš„åŒ»ç–—LLM**ï¼ˆå¦‚[ChatLaw](https://github.com/PKU-YuanGroup/ChatLaw)ï¼‰ï¼›
5. [BLOOMZ](https://huggingface.co/bigscience/bloomz)æ¨¡å‹ç³»åˆ—ä½¿ç”¨äº†PILEè¯­æ–™åº“è¿›è¡Œè®­ç»ƒï¼Œè¯¥è¯­æ–™åº“åŒ…å«å„ç§åŒ»å­¦æ–‡æœ¬ï¼ŒåŒ…æ‹¬`PubMed Central`å’Œ`PubMed Abstracts`ç­‰ã€‚è¿™äº›å®è´µçš„æ–‡æœ¬æå¤§åœ°ä¸°å¯Œäº†BLOOMZæ¨¡å‹çš„åŒ»å­¦çŸ¥è¯†ä½“ç³»ï¼Œæ‰€ä»¥å¾ˆå¤šå¼€æºé¡¹ç›®éƒ½ä¼šä¼˜å…ˆé€‰æ‹©BLOOMZåšåŒ»å­¦å¾®è°ƒçš„åº•åº§æ¨¡å‹ï¼›
6. (2023.08.26) ChatGPTåŸºäºä»£ç GPTè®­ç»ƒè€Œæ¥ï¼Œé‚£æˆ‘ä»¬é‡‡ç”¨[CodeLLaMA](https://huggingface.co/codellama)åœ¨ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒä¼šä¸ä¼šæ¯”åœ¨LLaMA-1/2ä¸Šå¾®è°ƒå–å¾—æ›´å¥½çš„ç»“æœå‘¢ï¼Ÿ
7. ç»“åˆæˆ‘ä»¬æœ€è¿‘çš„å·¥ä½œä¸æœ€è¿‘è®¸å¤šå…¬å¼€å‘è¡¨çš„å·¥ä½œè¯æ˜ï¼šåœ¨LLMæ—¶ä»£ï¼Œæ•°æ®`è´¨é‡ > æ•°é‡`è¿™ä¸ªçœŸç†ï¼Œå¦‚ï¼š[Less is More! ä¸Šäº¤æ¸…æº && é‡Œæµ· | åˆ©ç”¨200æ¡æ•°æ®å¾®è°ƒæ¨¡å‹ï¼Œæ€’è¶…MiniGPT-4ï¼](https://mp.weixin.qq.com/s/vbca2Y5LKqnOYnvEqqrgzQ)ï¼Œè¶…å¤§è§„æ¨¡çš„SFTæ•°æ®ä¼šè®©ä¸‹æ¸¸ä»»åŠ¡LLMå‡å¼±æˆ–è€…å¤±å»ICLã€CoTç­‰èƒ½åŠ›ï¼›
8. å¯¹äºå‚ç±»æ¨¡å‹ï¼Œæˆ–è®¸æˆ‘ä»¬æ›´åº”è¯¥å…³æ³¨PTçš„è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯é‡‡é›†åƒä¸‡ç™¾ä¸‡çš„SFTæ•°æ®åšè®­ç»ƒï¼Œæˆ‘ä»¬çš„å»ºè®®æ˜¯`å¤§è§„æ¨¡é¢„è®­ç»ƒ+å°è§„æ¨¡ç›‘ç£å¾®è°ƒ=è¶…å¼ºçš„LLMæ¨¡å‹`ï¼›
9. ä¸€ä¸ªå¥½çš„é¢„è®­ç»ƒåŒ»å­¦LLMå°šæœªåœ¨å¼€æºç¤¾åŒºä¸­è¢«å¼€æ”¾å‡ºæ¥ï¼ŒæœŸå¾…æœ‰äººèƒ½å»è¡¥å……è¿™æ ·çš„å·¥ä½œï¼›
10. é¢„è®­ç»ƒå¯ä»¥çŒå…¥çŸ¥è¯†ï¼Œç›‘ç£å¾®è°ƒåªæ˜¯æ¿€æ´»é¢†åŸŸèƒ½åŠ›(æ— æ³•å…³æ³¨çŸ¥è¯†)ï¼Ÿé¢„è®­ç»ƒçš„çŸ¥è¯†ä¸ç›‘ç£å¾®è°ƒçŸ¥è¯†åº”è¯¥å‘¼åº”ï¼Ÿé¢„è®­ç»ƒå‡ åGBçš„è¯­æ–™çŸ¥è¯†ä¼šè¢«åŸæ¥æ•°ä¸‡äº¿tokené¢„è®­ç»ƒçš„æ¨¡å‹çŸ¥è¯†æ·¹æ²¡ï¼Ÿ

> [!IMPORTANT]
> æ¬¢è¿å¤§å®¶åœ¨[ISSUE](https://github.com/WangRongsheng/CareLlama/issues/new)ä¸­è¡¥å……æ–°çš„ç»éªŒï¼

# ğŸ§°æ¨¡å‹å¼€æº

|é˜¶æ®µ|æƒé‡ä»‹ç»|ä¸‹è½½åœ°å€|ç‰¹ç‚¹|åº•åº§æ¨¡å‹|å¾®è°ƒæ–¹æ³•|æ•°æ®é›†|
|:-|:-|:-|:-|:-|:-|:-|
|ç›‘ç£å¾®è°ƒ|å¤šè½®å¯¹è¯æ•°æ®åŸºäºLLaMA2-7b-Chatè®­ç»ƒè€Œæ¥|[âš™ï¸CareLlama2-7b-chat-sft-multi](https://huggingface.co/wangrongsheng/CareLlama2-7b-chat-sft-multi)ã€[ğŸ§°CareLlama2-7b-multi](https://huggingface.co/wangrongsheng/CareLlama2-7b-multi)|å‡ºè‰²çš„å¤šè½®å¯¹è¯èƒ½åŠ›|LLaMA2-7b-Chat|QLoRA|mm|
|ç›‘ç£å¾®è°ƒ|ä¸°å¯Œé«˜æ•ˆåŒ»æ‚£å¯¹è¯æ•°æ®åŸºäºLLaMA2-7b-Chatè®­ç»ƒè€Œæ¥|[âš™ï¸CareLlama2-7b-chat-sft-med](https://huggingface.co/wangrongsheng/CareLlama2-7b-chat-sft-med)|å‡ºè‰²çš„æ‚£è€…ç–¾ç—…è¯Šæ–­èƒ½åŠ›|LLaMA2-7b-Chat|QLoRA|hm|
|ç›‘ç£å¾®è°ƒ|æ··åˆæ•°æ®åŸºäºLLaMA-7bè®­ç»ƒè€Œæ¥|[âš™ï¸CareLlama1-7b-merge](https://huggingface.co/wangrongsheng/CareLlama1-7b-merge)|æ›´å‡ºè‰²çš„åŒ»ç–—å¯¹è¯èƒ½åŠ›|LLaMA-7b|LoRA|mm,hm|
|ç›‘ç£å¾®è°ƒ|æ··åˆæ•°æ®åŸºäºLLaMA2-7b-Chatè®­ç»ƒè€Œæ¥|[âš™ï¸CareLlama2-7b-merge](https://huggingface.co/wangrongsheng/CareLlama2-7b-merge)ã€[ğŸ§°CareLlama2-7b-merge-mix](https://huggingface.co/wangrongsheng/CareLlama2-7b-merge-mix)|æ›´å‡ºè‰²çš„åŒ»ç–—å¯¹è¯èƒ½åŠ›|LLaMA2-7b-Chat|QLoRA|mm,hm|
|DPO||[âš™ï¸CareLlama2-7b-merge-dpo](https://huggingface.co/wangrongsheng/CareLlama2-7b-merge-dpo)||||rlhf|
|ç›‘ç£å¾®è°ƒ|æ›´å¤šæ··åˆæ•°æ®åŸºäºLLaMA2-7b-Chatè®­ç»ƒè€Œæ¥|[âš™ï¸CareLlama2-7b-super](https://huggingface.co/wangrongsheng/CareLlama2-7b-super)ã€[ğŸ§°CareLlama2-7b-super-mix](https://huggingface.co/wangrongsheng/CareLlama2-7b-super-mix)|æ›´å‡ºè‰²çš„åŒ»ç–—å¯¹è¯èƒ½åŠ›|LLaMA2-7b-Chat|QLoRA|mm,ls,ks,mc,ms,qz,hm|

> *ä½¿ç”¨æ–¹æ³•*ï¼š
> 1. ä¸‹è½½ç›¸åº”çš„åº•åº§æ¨¡å‹ï¼›
> 2. å¦‚æœä¸ºLLaMAåˆ™[è½¬ä¸ºHFæ ¼å¼](https://github.com/WangRongsheng/CareLlama#1%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96)ï¼Œå¦‚æœä¸ºLLaMA-2ä¸”ä¸‹è½½çš„ä¸ºHFæ ¼å¼åˆ™ä¸éœ€è¦è½¬åŒ–ï¼›
> 3. ä¸‹è½½ä¸Šè¿°ä½ æƒ³è¦åŠ è½½çš„æƒé‡ï¼›
> 4. æ ¹æ®[æ¨ç†é…ç½®](https://github.com/WangRongsheng/CareLlama/tree/main#4%E6%8E%A8%E7%90%86%E9%85%8D%E7%BD%AE)å¼€å§‹ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ï¼›

# ğŸ“³ç»“æœæ¼”ç¤º

![](./assets/examples/demo4.png)

<details>
<summary>æŸ¥çœ‹æ›´å¤šæ¼”ç¤º</summary>

![](./assets/examples/demo1.png)
![](./assets/examples/demo2.png)
![](./assets/examples/demo3.png)
![](./assets/examples/demo5.png)
![](./assets/examples/demo6.png)

</details>

æ›´å¤šç¤ºä¾‹ç»“æœè¯·çœ‹[CareLlama/discussions](https://huggingface.co/spaces/wangrongsheng/CareLlama/discussions)

# ğŸ°å…è´£å£°æ˜

æœ¬é¡¹ç›®ç›¸å…³èµ„æºä»…ä¾›å­¦æœ¯ç ”ç©¶ä¹‹ç”¨ï¼Œä¸¥ç¦ç”¨äºå•†ä¸šç”¨é€”ã€‚ä½¿ç”¨æ¶‰åŠç¬¬ä¸‰æ–¹ä»£ç çš„éƒ¨åˆ†æ—¶ï¼Œè¯·ä¸¥æ ¼éµå¾ªç›¸åº”çš„å¼€æºåè®®ã€‚æ¨¡å‹ç”Ÿæˆçš„å†…å®¹å—æ¨¡å‹è®¡ç®—ã€éšæœºæ€§å’Œé‡åŒ–ç²¾åº¦æŸå¤±ç­‰å› ç´ å½±å“ï¼Œæœ¬é¡¹ç›®æ— æ³•å¯¹å…¶å‡†ç¡®æ€§ä½œå‡ºä¿è¯ã€‚å³ä½¿æœ¬é¡¹ç›®æ¨¡å‹è¾“å‡ºç¬¦åˆåŒ»å­¦äº‹å®ï¼Œä¹Ÿä¸èƒ½è¢«ç”¨ä½œå®é™…åŒ»å­¦è¯Šæ–­çš„ä¾æ®ã€‚å¯¹äºæ¨¡å‹è¾“å‡ºçš„ä»»ä½•å†…å®¹ï¼Œæœ¬é¡¹ç›®ä¸æ‰¿æ‹…ä»»ä½•æ³•å¾‹è´£ä»»ï¼Œäº¦ä¸å¯¹å› ä½¿ç”¨ç›¸å…³èµ„æºå’Œè¾“å‡ºç»“æœè€Œå¯èƒ½äº§ç”Ÿçš„ä»»ä½•æŸå¤±æ‰¿æ‹…è´£ä»»ã€‚

# ğŸ¥‚é¡¹ç›®å¼•ç”¨

æœ¬å·¥ä½œç”±æ¾³é—¨ç†å·¥å¤§å­¦åº”ç”¨ç§‘å­¦å­¦é™¢ç¡•å£«ç ”ç©¶ç”Ÿç‹è£èƒœã€å‘¨ç‘å“²ã€é™ˆæµ©æ˜å®Œæˆï¼ŒæŒ‡å¯¼è€å¸ˆä¸ºæª€éŸ¬å‰¯æ•™æˆå’Œç‹äºšé¹å‰¯æ•™æˆã€‚

å¦‚æœä½ ä½¿ç”¨äº†æœ¬é¡¹ç›®çš„æ¨¡å‹ï¼Œæ•°æ®æˆ–è€…ä»£ç ï¼Œè¯·å£°æ˜å¼•ç”¨ï¼š
```bib
@misc{wang2023carellama,
      title={CareLlama: Medical LLM, Open Source Driven for a Healthy Future}, 
      author={Rongsheng Wang, Ruizhe Zhou, Haoming Chen, Yapeng Wang, Tao Tan},
      year={2023},
      publisher = {GitHub},
      journal = {GitHub repository},
      howpublished = {\url{https://github.com/WangRongsheng/CareLlama}},
}
```

```bib
@article{wang2023ivygpt,
  title={IvyGPT: InteractiVe Chinese pathwaY language model in medical domain},
  author={Wang, Rongsheng and Duan, Yaofei and Lam, ChanTong and Chen, Jiexi and Xu, Jiangsheng and Chen, Haoming and Liu, Xiaohong and Pang, Patrick Cheong-Iao and Tan, Tao},
  journal={arXiv preprint arXiv:2307.10512},
  year={2023}
}
```

# ğŸ””ä½¿ç”¨è®¸å¯

æ­¤å­˜å‚¨åº“éµå¾ª[MIT License](https://github.com/WangRongsheng/CareLlama/blob/main/LICENSE) ï¼Œè¯·å‚é˜…è®¸å¯æ¡æ¬¾ã€‚

# ğŸ—ï¸èµåŠ©æ”¯æŒ

å¦‚æœè§‰å¾—è¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œå¹¶ä¸”æ„¿æ„æ”¯æŒæˆ‘ä»¬çš„å·¥ä½œï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ï¼ˆ**è¯·å¤‡æ³¨æ‚¨çš„å¾®ä¿¡ç»™æˆ‘ä»¬**ï¼‰ï¼š

|||
|:-|:-|
|<img src="./assets/support/wx.jpg" width="200" />|<img src="./assets/support/zfb.jpg" width="200" />|

æ‚¨çš„æ”¯æŒå°†æ˜¯æˆ‘ä»¬ç»§ç»­æ¢ç´¢LLMçš„åŠ¨åŠ›ï¼Œæ‰€æœ‰çš„æ”¯æŒå°†ç”¨äº**æ¨¡å‹çš„æ¨ç†éƒ¨ç½²ä»»åŠ¡**ä¸­ã€‚

# ğŸ“šé¡¹ç›®å‚è€ƒ

#### åŒ»å­¦LLM
- https://github.com/llSourcell/DoctorGPT
- https://github.com/facebookresearch/llama-recipes
- https://github.com/Kent0n-Li/ChatDoctor
- https://github.com/michael-wzhu/ShenNong-TCM-LLM
- https://github.com/michael-wzhu/ChatMed
- https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese
- https://github.com/SCIR-HI/Med-ChatGLM
- https://github.com/xionghonglin/DoctorGLM
- https://github.com/MediaBrain-SJTU/MING
- https://github.com/CMKRG/QiZhenGPT
- https://github.com/NLPxiaoxu/LLM-Pretrain-FineTune
- https://github.com/scutcyr/BianQue
- https://github.com/thomas-yanxin/Sunsimiao
- https://github.com/kbressem/medAlpaca
- https://github.com/FreedomIntelligence/HuatuoGPT
- https://github.com/shibing624/MedicalGPT
- https://github.com/chaoyi-wu/PMC-LLaMA
- https://github.com/pariskang/CMLM-ZhongJing
- https://github.com/SupritYoung/Zhongjing
- https://github.com/openmedlab/PULSE
- https://github.com/FudanDISC/DISC-MedLLM
- https://github.com/Zlasejd/HuangDI
- https://github.com/2020MEAI/TCMLLM
- https://github.com/PharMolix/OpenBioMed
- https://huggingface.co/Writer/palmyra-med-20b
- https://github.com/winninghealth/WiNGPT2

#### è¯„æµ‹LLM

- https://github.com/FreedomIntelligence/CMB

#### ä½“éªŒLLM

- https://medical.chat-data.com/
- http://med.fudan-disc.com/
- https://www.huatuogpt.cn/
- https://huggingface.co/spaces/wangrongsheng/CareLlama
- ([password](https://huggingface.co/fb700/chatglm-fitness-RLHF))https://huggingface.co/spaces/fb700/chatglm-fitness-RLHF

#### éƒ¨ç½²LLM
- https://github.com/a16z-infra/llama2-chatbot
- https://github.com/liltom-eth/llama2-webui
- https://github.com/soulteary/docker-llama2-chat
- https://huggingface.co/spaces/LinkSoul/Chinese-Llama-2-7b
- https://github.com/mushan0x0/AI0x0.com
- https://github.com/Yidadaa/ChatGPT-Next-Web
- https://github.com/sunner/ChatALL
- https://github.com/chatchat-space/Langchain-Chatchat
- https://github.com/wenda-LLM/wenda
- https://github.com/xusenlinzy/api-for-open-llm
- https://github.com/yuanjie-ai/ChatLLM
- https://github.com/labring/FastGPT
- https://github.com/vllm-project/vllm
- https://github.com/dataelement/bisheng
- https://github.com/lobehub/lobe-chat
- https://github.com/purton-tech/bionicgpt
- https://github.com/Chainlit/chainlit

#### LLMæ•°æ®åˆ¶ä½œ

- https://github.com/yanqiangmiffy/GoGPT-Instruction
- https://github.com/wpydcr/LLM-Kit
- https://github.com/huang1332/finetune_dataset_maker
- https://github.com/threeColorFr/LLMforDialogDataGenerate
- https://github.com/alibaba/data-juicer

#### LLMèµ„æº
- https://github.com/onejune2018/Awesome-Medical-Healthcare-Dataset-For-LLM
- https://github.com/WangRongsheng/MedQA-ChatGLM
- https://github.com/hiyouga/LLaMA-Efficient-Tuning
- https://github.com/WangRongsheng/Use-LLMs-in-Colab
- https://github.com/HqWu-HITCS/Awesome-Chinese-LLM
- https://github.com/LearnPrompt/LLMs-cookbook
- https://github.com/liucongg/ChatGPTBook
- https://github.com/EvilPsyCHo/train_custom_LLM


![](./assets/images/end.png)
