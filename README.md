# ğŸæ•°æ®é›†

#### é¢„è®­ç»ƒæ•°æ®

#### ç›‘ç£è®­ç»ƒæ•°æ®
- [icliniq-10k(en)](https://drive.google.com/file/d/1ZKbqgYqWc7DJHs3N9TQYQVPdDQmZaClA/view?usp=sharing)
- [HealthCareMagic-100k (en)](https://drive.google.com/file/d/1lyfqIwlLSClhgrCutWuEe_IACNq6XNUt/view?usp=sharing)

#### å¥–åŠ±è®­ç»ƒæ•°æ®

# ğŸ—œï¸å…¨æµç¨‹è®­ç»ƒ

## 1.å®‰è£…ä¾èµ–

```python
conda create -n llm python=3.11
conda activate llm
python -m pip install -r requirements.txt
```

## 2.æ•°æ®é…ç½®

<details>
<summary>æ•°æ®é›†é…ç½®ã€PTã€SFTã€RWæ•°æ®æ ¼å¼è¯¦ç»†å†…å®¹</summary>

### dataset_info

å¦‚æœæ‚¨ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†ï¼Œè¯·åŠ¡å¿…åœ¨ `dataset_info.json` æ–‡ä»¶ä¸­ä»¥å¦‚ä¸‹æ ¼å¼æä¾›æ‚¨çš„æ•°æ®é›†å®šä¹‰ã€‚

```json
"æ•°æ®é›†åç§°": {
  "hf_hub_url": "HuggingFaceä¸Šçš„é¡¹ç›®åœ°å€ï¼ˆè‹¥æŒ‡å®šï¼Œåˆ™å¿½ç•¥ä¸‹åˆ—ä¸‰ä¸ªå‚æ•°ï¼‰",
  "script_url": "åŒ…å«æ•°æ®åŠ è½½è„šæœ¬çš„æœ¬åœ°æ–‡ä»¶å¤¹åç§°ï¼ˆè‹¥æŒ‡å®šï¼Œåˆ™å¿½ç•¥ä¸‹åˆ—ä¸¤ä¸ªå‚æ•°ï¼‰",
  "file_name": "è¯¥ç›®å½•ä¸‹æ•°æ®é›†æ–‡ä»¶çš„åç§°ï¼ˆè‹¥ä¸Šè¿°å‚æ•°æœªæŒ‡å®šï¼Œåˆ™æ­¤é¡¹å¿…éœ€ï¼‰",
  "file_sha1": "æ•°æ®é›†æ–‡ä»¶çš„SHA-1å“ˆå¸Œå€¼ï¼ˆå¯é€‰ï¼‰",
  "columns": {
    "prompt": "æ•°æ®é›†ä»£è¡¨æç¤ºè¯çš„è¡¨å¤´åç§°ï¼ˆé»˜è®¤ï¼šinstructionï¼‰",
    "query": "æ•°æ®é›†ä»£è¡¨è¯·æ±‚çš„è¡¨å¤´åç§°ï¼ˆé»˜è®¤ï¼šinputï¼‰",
    "response": "æ•°æ®é›†ä»£è¡¨å›ç­”çš„è¡¨å¤´åç§°ï¼ˆé»˜è®¤ï¼šoutputï¼‰",
    "history": "æ•°æ®é›†ä»£è¡¨å†å²å¯¹è¯çš„è¡¨å¤´åç§°ï¼ˆé»˜è®¤ï¼šNoneï¼‰"
  }
}
```

å…¶ä¸­ `prompt` å’Œ `response` åˆ—åº”å½“æ˜¯éç©ºçš„å­—ç¬¦ä¸²ã€‚`query` åˆ—çš„å†…å®¹å°†ä¼šå’Œ `prompt` åˆ—æ‹¼æ¥ä½œä¸ºæ¨¡å‹è¾“å…¥ã€‚`history` åˆ—åº”å½“æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²äºŒå…ƒç»„ï¼Œåˆ†åˆ«ä»£è¡¨ç”¨æˆ·è¯·æ±‚å’Œæ¨¡å‹ç­”å¤ã€‚

### PT example data

`.txt`æ ¼å¼ï¼Œä¸€è¡Œä¸€ä¸ªæ— ç›‘ç£æ•°æ®ã€‚

```html
Machine learning (ML) is a field devoted to understanding and building methods that let machines "learn" â€“ that is, methods that leverage data to improve computer performance on some set of tasks.
Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, agriculture, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.
```

### SFT example data 1

```json
[
  {
    "instruction": "å¬èµ·æ¥å¾ˆä¸é”™ã€‚äººå·¥æ™ºèƒ½å¯èƒ½åœ¨å“ªäº›æ–¹é¢é¢ä¸´æŒ‘æˆ˜å‘¢ï¼Ÿ",
    "input": "",
    "output": "äººå·¥æ™ºèƒ½é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬æ•°æ®éšç§ã€å®‰å…¨å’Œé“å¾·æ–¹é¢çš„é—®é¢˜ï¼Œä»¥åŠå½±å“å°±ä¸šæœºä¼šçš„è‡ªåŠ¨åŒ–ç­‰é—®é¢˜ã€‚",
    "history": [
      ["ä½ å¥½ï¼Œä½ èƒ½å¸®æˆ‘è§£ç­”ä¸€ä¸ªé—®é¢˜å—ï¼Ÿ", "å½“ç„¶ï¼Œè¯·é—®æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ"],
      ["æˆ‘æƒ³äº†è§£äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘ï¼Œä½ æœ‰ä»€ä¹ˆæƒ³æ³•å—ï¼Ÿ", "äººå·¥æ™ºèƒ½åœ¨æœªæ¥çš„å‘å±•æ–¹å‘å¯èƒ½åŒ…æ‹¬æ›´å¼ºå¤§çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ›´å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œä»¥åŠæ›´åŠ æ™ºèƒ½çš„æœºå™¨äººã€‚"]
    ]
  }
]
```

### SFT example data 2

```json
[
  {
    "instruction": "å¬èµ·æ¥å¾ˆä¸é”™ã€‚äººå·¥æ™ºèƒ½å¯èƒ½åœ¨å“ªäº›æ–¹é¢é¢ä¸´æŒ‘æˆ˜å‘¢ï¼Ÿ",
    "input": "",
    "output": "äººå·¥æ™ºèƒ½é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬æ•°æ®éšç§ã€å®‰å…¨å’Œé“å¾·æ–¹é¢çš„é—®é¢˜ï¼Œä»¥åŠå½±å“å°±ä¸šæœºä¼šçš„è‡ªåŠ¨åŒ–ç­‰é—®é¢˜ã€‚",
    "history": []
  }
]
```

### RW example data

```json
[
  {
    "instruction": "ç”Ÿæˆä¸‰ä¸ªä¸â€œé“æ­‰â€æ„æ€ç›¸åŒçš„åŠ¨è¯",
    "input": "",
    "output": [
      "æ‰¿è®¤ï¼Œè¡¨ç¤ºé—æ†¾ï¼Œå¼¥è¡¥ã€‚",
      "é“æ­‰"
    ]
  }
]
```
  
</details>

## 3.è®­ç»ƒé…ç½®

<details>
<summary>è®­ç»ƒå‚æ•°ä¸æŒ‡ä»¤</summary>

### é…ç½®åˆ†å¸ƒå¼

```python
accelerate config # configure the environment
accelerate launch src/train_bash.py # arguments (same as above)
```

### ç›‘ç£è®­ç»ƒ

```python
accelerate launch src/train_bash.py \
    --stage sft \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --do_train \
    --dataset mm \
    --finetuning_type lora \
    --quantization_bit 4 \
    --overwrite_cache \
    --output_dir output \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 5e-5 \
    --num_train_epochs 2.0 \
    --plot_loss \
    --fp16 \
    --template llama2 \
    --lora_target q_proj,v_proj
```
  
</details>

# ğŸ“šå‚è€ƒ

- https://github.com/llSourcell/DoctorGPT
- https://github.com/facebookresearch/llama-recipes
- https://github.com/Kent0n-Li/ChatDoctor
- https://github.com/hiyouga/LLaMA-Efficient-Tuning
