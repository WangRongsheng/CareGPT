> **Note**
>
> åœ¨çº¿ä½“éªŒCareLlamaï¼šhttps://huggingface.co/spaces/wangrongsheng/CareLlama

<div align="center">
  <a href="https://github.com/WangRongsheng/ChatGenTitle">
    <img src="https://github.com/WangRongsheng/CareLlama/blob/main/assets/images/home.png" alt="Logo" height="280">
  </a>

  <p align="center">
    <h3> CareLlama (å…³æ€€ç¾Šé©¼)ï¼šåŒ»ç–—LLMï¼Œå¼€æºé©±åŠ¨ï¼Œå…±åˆ›å¥åº·æœªæ¥ </h3>
    <p align="center">
      <em>èµ„æºæ•´åˆ / å¼€æºæ¨¡å‹ / ä¸°å¯Œæ•°æ® / é«˜æ•ˆéƒ¨ç½² / LLaMA</em>
    </p>
    <p align="center">
      <a href='https://github.com/WangRongsheng/CareLlama'>
            <img src='https://img.shields.io/badge/Project-Page-Green'>
      </a>
      <a href='https://github.com/WangRongsheng/CareLlama'>
            <img src='https://img.shields.io/badge/Paper-Arxiv-red'>
      </a>
      <a href="#">
        <img alt="GitHub Contributors" src="https://colab.research.google.com/assets/colab-badge.svg" />
      </a>
      <a href='https://huggingface.co/wangrongsheng'>
        <img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue'>
      </a>
      </br>
      <a href="https://github.com/WangRongsheng/CareLlama/graphs/contributors">
        <img alt="GitHub Contributors" src="https://img.shields.io/github/contributors/WangRongsheng/CareLlama" />
      </a>
      <a href="https://github.com/WangRongsheng/CareLlama/issues">
        <img alt="Issues" src="https://img.shields.io/github/issues/WangRongsheng/CareLlama?color=0088ff" />
      </a>
      <a href="https://github.com/WangRongsheng/CareLlama/pulls">
        <img alt="GitHub pull requests" src="https://img.shields.io/github/issues-pr/WangRongsheng/CareLlama?color=0088ff" />
      </a>
      <a href=href="https://github.com/WangRongsheng/CareLlama/stargazers">
        <img src="https://img.shields.io/github/stars/WangRongsheng/CareLlama?color=ccf">
      </a>
      <a href=href="https://github.com/WangRongsheng/CareLlama">
        <img src="https://img.shields.io/github/repo-size/WangRongsheng/CareLlama.svg?style=flat-square">
      </a>
      </br>
      <a href=href="https://github.com/WangRongsheng/CareLlama">
        <img src="https://visitor-badge.laobi.icu/badge?page_id=https://github.com/WangRongsheng/CareLlama">
      </a>
      <a href=href="https://github.com/WangRongsheng/CareLlama">
        <img src="https://img.shields.io/github/last-commit/WangRongsheng/CareLlama">
      </a>
      <a href="https://github.com/WangRongsheng/CareLlama/blob/main/LICENSE">
        <img alt="GitHub Contributors" src="https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg" />
      </a>
  </p>
</div>

<!--center><kbd><img src="./docs/images/usage.png" height="550px"/></kbd></center-->

<p align="center">
      <a href="#"><strong>è§†é¢‘æ•™ç¨‹</strong></a>
      <a href="#"><strong>å®‰è£…éƒ¨ç½²</strong></a>
      <a href="#"><strong>åœ¨çº¿ä½“éªŒ</strong></a>
</p>

![](./assets/images/hx.png)

æ›´æ–°å†å²ï¼š
- *âš¡2023.08.25*: æ­£å¼å¼€æºCareLlamaï¼›

# ğŸæ•°æ®é›†

#### é¢„è®­ç»ƒæ•°æ®

- [LLM-Pretrain-FineTune/data_pretrain](https://github.com/X-jun-0130/LLM-Pretrain-FineTune/tree/main/data_pretrain)
- [MedicalGPT/pretrain](https://github.com/shibing624/MedicalGPT/tree/main/data/pretrain)
- [zysj](https://www.zysj.com.cn/)

#### ç›‘ç£è®­ç»ƒæ•°æ®
- [icliniq-10k(en)](https://drive.google.com/file/d/1ZKbqgYqWc7DJHs3N9TQYQVPdDQmZaClA/view?usp=sharing)
- [HealthCareMagic-100k (en)](https://drive.google.com/file/d/1lyfqIwlLSClhgrCutWuEe_IACNq6XNUt/view?usp=sharing)
- [ShenNong_TCM_Dataset](https://huggingface.co/datasets/michaelwzhu/ShenNong_TCM_Dataset)
- [ChatMed_Consult_Dataset](https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset)
- [Chinese-medical-dialogue-data](https://github.com/Toyhom/Chinese-medical-dialogue-data)
- [cMedQA2](https://github.com/zhangsheng93/cMedQA2)
- [Huatuo-26M](https://github.com/FreedomIntelligence/Huatuo-26M)
- [cMedQA2](https://github.com/zhangsheng93/cMedQA2)
- [webMedQA](https://github.com/hejunqing/webMedQA)
- [PubMedQA](https://pubmedqa.github.io/)
- [CMCQA](https://github.com/WENGSYX/CMCQA)
- [QiZhenGPT](https://github.com/CMKRG/QiZhenGPT/tree/main/data)
- [LLM-Pretrain-FineTune/data_sft](https://github.com/X-jun-0130/LLM-Pretrain-FineTune/tree/main/data_sft)
- [Medical-Dialogue-System](https://github.com/UCSD-AI4H/Medical-Dialogue-System)
- [IMCS-V2](https://github.com/lemuria-wchen/imcs21)
- [CHIP-MDCFNPC](https://tianchi.aliyun.com/dataset/95414)
- [MedDG](https://tianchi.aliyun.com/dataset/95414)
- [HuatuoGPT-sft-data-v1](https://huggingface.co/datasets/FreedomIntelligence/HuatuoGPT-sft-data-v1)
- [MedicalGPT/finetune](https://github.com/shibing624/MedicalGPT/tree/main/data/finetune)
- [shibing624/medical](https://huggingface.co/datasets/shibing624/medical)
- [medAlpaca/data](https://github.com/kbressem/medAlpaca#data-overview)
- [Zhongjing/sft](https://github.com/SupritYoung/Zhongjing/tree/main/data)
- [medical_dialog](https://huggingface.co/datasets/medical_dialog)
- [huatuo_encyclopedia_qa](https://huggingface.co/datasets/FreedomIntelligence/huatuo_encyclopedia_qa)
- [Chinese-medical-dialogue-data](https://huggingface.co/datasets/BillGPT/Chinese-medical-dialogue-data)
- [Med-ChatGLM/data](https://github.com/SCIR-HI/Med-ChatGLM/tree/main/data)
- [CMB](https://github.com/FreedomIntelligence/CMB)
- [GenMedGPT-5k](https://drive.google.com/file/d/1nDTKZ3wZbZWTkFMBkxlamrzbNz0frugg/view?usp=sharing)

#### å¥–åŠ±è®­ç»ƒæ•°æ®

- [MedicalGPT/reward](https://github.com/shibing624/MedicalGPT/tree/main/data/reward)
- [Zhongjing/rw](https://github.com/SupritYoung/Zhongjing/tree/main/data)
- [comparison_gpt4_data](https://huggingface.co/datasets/wangrongsheng/comparison_gpt4_data)
- [HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf)

# ğŸ—œï¸å…¨æµç¨‹è®­ç»ƒ

## 1.å®‰è£…ä¾èµ–

```python
conda create -n llm python=3.11
conda activate llm
python -m pip install -r requirements.txt
```

- LLaMAæ¨¡å‹ä¸‹è½½ï¼šhttps://blog.csdn.net/u014297502/article/details/129829677
```python
# è½¬ä¸ºHFæ ¼å¼
python -m transformers.models.llama.convert_llama_weights_to_hf \
    --input_dir path_to_llama_weights --model_size 7B --output_dir path_to_llama_model
```
- LLaMA-2æ¨¡å‹ä¸‹è½½ï¼šhttps://huggingface.co/meta-llama

## 2.æ•°æ®é…ç½®

<details>
<summary>æ•°æ®é›†é…ç½®ã€PTã€SFTã€RWæ•°æ®æ ¼å¼</summary>

### dataset_info

å¦‚æœæ‚¨ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†ï¼Œè¯·åŠ¡å¿…åœ¨ `dataset_info.json` æ–‡ä»¶ä¸­ä»¥å¦‚ä¸‹æ ¼å¼æä¾›æ‚¨çš„æ•°æ®é›†å®šä¹‰ã€‚

```json
"æ•°æ®é›†åç§°": {
  "hf_hub_url": "HuggingFaceä¸Šçš„é¡¹ç›®åœ°å€ï¼ˆè‹¥æŒ‡å®šï¼Œåˆ™å¿½ç•¥ä¸‹åˆ—ä¸‰ä¸ªå‚æ•°ï¼‰",
  "script_url": "åŒ…å«æ•°æ®åŠ è½½è„šæœ¬çš„æœ¬åœ°æ–‡ä»¶å¤¹åç§°ï¼ˆè‹¥æŒ‡å®šï¼Œåˆ™å¿½ç•¥ä¸‹åˆ—ä¸¤ä¸ªå‚æ•°ï¼‰",
  "file_name": "è¯¥ç›®å½•ä¸‹æ•°æ®é›†æ–‡ä»¶çš„åç§°ï¼ˆè‹¥ä¸Šè¿°å‚æ•°æœªæŒ‡å®šï¼Œåˆ™æ­¤é¡¹å¿…éœ€ï¼‰",
  "file_sha1": "æ•°æ®é›†æ–‡ä»¶çš„SHA-1å“ˆå¸Œå€¼ï¼ˆå¯é€‰ï¼‰",
  "columns": {
    "prompt": "æ•°æ®é›†ä»£è¡¨æç¤ºè¯çš„è¡¨å¤´åç§°ï¼ˆé»˜è®¤ï¼šinstructionï¼‰",
    "query": "æ•°æ®é›†ä»£è¡¨è¯·æ±‚çš„è¡¨å¤´åç§°ï¼ˆé»˜è®¤ï¼šinputï¼‰",
    "response": "æ•°æ®é›†ä»£è¡¨å›ç­”çš„è¡¨å¤´åç§°ï¼ˆé»˜è®¤ï¼šoutputï¼‰",
    "history": "æ•°æ®é›†ä»£è¡¨å†å²å¯¹è¯çš„è¡¨å¤´åç§°ï¼ˆé»˜è®¤ï¼šNoneï¼‰"
  }
}
```

å…¶ä¸­ `prompt` å’Œ `response` åˆ—åº”å½“æ˜¯éç©ºçš„å­—ç¬¦ä¸²ã€‚`query` åˆ—çš„å†…å®¹å°†ä¼šå’Œ `prompt` åˆ—æ‹¼æ¥ä½œä¸ºæ¨¡å‹è¾“å…¥ã€‚`history` åˆ—åº”å½“æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²äºŒå…ƒç»„ï¼Œåˆ†åˆ«ä»£è¡¨ç”¨æˆ·è¯·æ±‚å’Œæ¨¡å‹ç­”å¤ã€‚

### PT example data

`.txt`æ ¼å¼ï¼Œä¸€è¡Œä¸€ä¸ªæ— ç›‘ç£æ•°æ®ã€‚

```html
Machine learning (ML) is a field devoted to understanding and building methods that let machines "learn" â€“ that is, methods that leverage data to improve computer performance on some set of tasks.
Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, agriculture, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.
```

### SFT example data 1

```json
[
  {
    "instruction": "å¬èµ·æ¥å¾ˆä¸é”™ã€‚äººå·¥æ™ºèƒ½å¯èƒ½åœ¨å“ªäº›æ–¹é¢é¢ä¸´æŒ‘æˆ˜å‘¢ï¼Ÿ",
    "input": "",
    "output": "äººå·¥æ™ºèƒ½é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬æ•°æ®éšç§ã€å®‰å…¨å’Œé“å¾·æ–¹é¢çš„é—®é¢˜ï¼Œä»¥åŠå½±å“å°±ä¸šæœºä¼šçš„è‡ªåŠ¨åŒ–ç­‰é—®é¢˜ã€‚",
    "history": [
      ["ä½ å¥½ï¼Œä½ èƒ½å¸®æˆ‘è§£ç­”ä¸€ä¸ªé—®é¢˜å—ï¼Ÿ", "å½“ç„¶ï¼Œè¯·é—®æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ"],
      ["æˆ‘æƒ³äº†è§£äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘ï¼Œä½ æœ‰ä»€ä¹ˆæƒ³æ³•å—ï¼Ÿ", "äººå·¥æ™ºèƒ½åœ¨æœªæ¥çš„å‘å±•æ–¹å‘å¯èƒ½åŒ…æ‹¬æ›´å¼ºå¤§çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ›´å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ï¼Œä»¥åŠæ›´åŠ æ™ºèƒ½çš„æœºå™¨äººã€‚"]
    ]
  }
]
```

### SFT example data 2

```json
[
  {
    "instruction": "å¬èµ·æ¥å¾ˆä¸é”™ã€‚äººå·¥æ™ºèƒ½å¯èƒ½åœ¨å“ªäº›æ–¹é¢é¢ä¸´æŒ‘æˆ˜å‘¢ï¼Ÿ",
    "input": "",
    "output": "äººå·¥æ™ºèƒ½é¢ä¸´çš„æŒ‘æˆ˜åŒ…æ‹¬æ•°æ®éšç§ã€å®‰å…¨å’Œé“å¾·æ–¹é¢çš„é—®é¢˜ï¼Œä»¥åŠå½±å“å°±ä¸šæœºä¼šçš„è‡ªåŠ¨åŒ–ç­‰é—®é¢˜ã€‚",
    "history": []
  }
]
```

### RW example data

```json
[
  {
    "instruction": "ç”Ÿæˆä¸‰ä¸ªä¸â€œé“æ­‰â€æ„æ€ç›¸åŒçš„åŠ¨è¯",
    "input": "",
    "output": [
      "æ‰¿è®¤ï¼Œè¡¨ç¤ºé—æ†¾ï¼Œå¼¥è¡¥ã€‚",
      "é“æ­‰"
    ]
  }
]
```
  
</details>

## 3.è®­ç»ƒé…ç½®

<details>
<summary>è®­ç»ƒå‚æ•°ä¸æŒ‡ä»¤</summary>

### é…ç½®åˆ†å¸ƒå¼

```python
accelerate config # configure the environment
accelerate launch src/train_bash.py # arguments (same as above)
```

### ç›‘ç£è®­ç»ƒ

```python
accelerate launch src/train_bash.py \
    --stage sft \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --do_train \
    --dataset mm \
    --finetuning_type lora \
    --quantization_bit 4 \
    --overwrite_cache \
    --output_dir output \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 4 \
    --lr_scheduler_type cosine \
    --logging_steps 10 \
    --save_steps 1000 \
    --learning_rate 5e-5 \
    --num_train_epochs 2.0 \
    --plot_loss \
    --fp16 \
    --template llama2 \
    --lora_target q_proj,v_proj
```
  
</details>

## 4.æ¨ç†é…ç½®

<details>
<summary>æ¨ç†å‚æ•°ä¸æŒ‡ä»¤</summary>

### Webè®¿é—®

```python
python src/web_demo.py \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --checkpoint_dir output \
    --finetuning_type lora \
    --template llama2
```

### APIè®¿é—®

```python
python src/api_demo.py \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --checkpoint_dir output \
    --finetuning_type lora \
    --template llama2
```

### CLIè®¿é—®

```python
python src/cli_demo.py \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --checkpoint_dir output \
    --finetuning_type lora \
    --template llama2
```

### æ‰¹é‡é¢„æµ‹

```python
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
    --stage sft \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --do_predict \
    --dataset mm \
    --template llama2 \
    --finetuning_type lora \
    --checkpoint_dir output \
    --output_dir predict_output \
    --per_device_eval_batch_size 8 \
    --max_samples 100 \
    --predict_with_generate
```

### å®éªŒè¯„ä¼°(BLEUå’ŒROUGE_CHINESE)

```python
CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \
    --stage sft \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --do_eval \
    --dataset mm \
    --template llama2 \
    --finetuning_type lora \
    --checkpoint_dir output \
    --output_dir eval_output \
    --per_device_eval_batch_size 8 \
    --max_samples 100 \
    --predict_with_generate
```

åœ¨4/8-bitè¯„ä¼°æ—¶ï¼Œæ¨èä½¿ç”¨`--per_device_eval_batch_size=1`å’Œ`--max_target_length 128`

</details>

## 5.Gradioéƒ¨ç½²

<details>
<summary>Gradioéƒ¨ç½²æŒ‡ä»¤</summary>

### æ¨¡å‹å¯¼å‡º

```python
python src/export_model.py \
    --model_name_or_path ./Llama-2-7b-chat-hf \
    --template llama2 \
    --finetuning_type lora \
    --checkpoint_dir output \
    --output_dir output_export
```

### å¼€å¯è¿è¡Œ

```python
%cd Gradio
python app.py
```

</details>

![](./Gradio/gradio-demo.png)

# ğŸ’«å®è·µç»éªŒ

1. åœ¨CareLlamaä¸­å¹¶æœªå¯¹åˆ†è¯æ¨¡å‹è¿›è¡Œä¸­æ–‡åˆ†è¯çš„æ·»åŠ å’Œé‡æ–°è®­ç»ƒï¼Œä½†æ˜¯æ•ˆæœä¾æ—§è¡¨ç°å¯å–œï¼›
2. å…¨æµç¨‹çš„LLMè®­ç»ƒåŒ…æ‹¬ï¼šé¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒã€å¥–åŠ±æ¨¡å‹ã€å¼ºåŒ–å­¦ä¹ ï¼Œ**å¤šæ•°æƒ…å†µä¸‹ç›‘ç£å¾®è°ƒå³å¯æ»¡è¶³è‡ªèº«éœ€æ±‚**ï¼›
3. åœ¨ç®—åŠ›å……è¶³æƒ…å†µä¸‹æ¨è**ä½¿ç”¨åŒ»ç–—æ•°æ®å’Œé€šç”¨è¯­æ–™æ•°æ®è¿›è¡Œè®­ç»ƒ**ï¼Œè¿™æ ·æ¨¡å‹æ—¢å¯ä»¥æœ‰åŒ»å­¦ä¸Šçš„è®­ç»ƒå­¦ä¹ ï¼Œä¹Ÿå¯ä»¥ä¿æŒé€šç”¨èƒ½åŠ›ï¼ˆå¦‚æŒ‡ä»¤éµå¾ªï¼‰ï¼›
4. ä¸è¦æŒ‡æœ›ä¸€ä¸ªåŒ»ç–—LLMå°±å¯ä»¥æ»¡è¶³æ‰€æœ‰éœ€æ±‚ï¼Œåˆç†çš„åšæ³•å¯èƒ½æ˜¯å®æ—¶æ›´æ–°çš„**çŸ¥è¯†åº“+å¾®è°ƒçš„åŒ»ç–—LLM**ï¼ˆå¦‚[ChatLaw](https://github.com/PKU-YuanGroup/ChatLaw)ï¼‰ï¼›
5. [BLOOMZ](https://huggingface.co/bigscience/bloomz)æ¨¡å‹ç³»åˆ—ä½¿ç”¨äº†PILEè¯­æ–™åº“è¿›è¡Œè®­ç»ƒï¼Œè¯¥è¯­æ–™åº“åŒ…å«å„ç§åŒ»å­¦æ–‡æœ¬ï¼ŒåŒ…æ‹¬`PubMed Central`å’Œ`PubMed Abstracts`ç­‰ã€‚è¿™äº›å®è´µçš„æ–‡æœ¬æå¤§åœ°ä¸°å¯Œäº†BLOOMZæ¨¡å‹çš„åŒ»å­¦çŸ¥è¯†ä½“ç³»ï¼Œæ‰€ä»¥å¾ˆå¤šå¼€æºé¡¹ç›®éƒ½ä¼šä¼˜å…ˆé€‰æ‹©BLOOMZåšåŒ»å­¦å¾®è°ƒçš„åº•åº§æ¨¡å‹ï¼›
6. (2023.08.26) ChatGPTåŸºäºä»£ç GPTè®­ç»ƒè€Œæ¥ï¼Œé‚£æˆ‘ä»¬é‡‡ç”¨[CodeLLaMA](https://huggingface.co/codellama)åœ¨ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒä¼šä¸ä¼šæ¯”åœ¨LLaMA-1/2ä¸Šå¾®è°ƒå–å¾—æ›´å¥½çš„ç»“æœå‘¢ï¼Ÿ

# ğŸ§°æ¨¡å‹å¼€æº

|é˜¶æ®µ|æƒé‡ä»‹ç»|ä¸‹è½½åœ°å€|ç‰¹ç‚¹|åº•åº§æ¨¡å‹|å¾®è°ƒæ–¹æ³•|
|:-|:-|:-|:-|:-|:-|
|ç›‘ç£å¾®è°ƒ|å¤šè½®å¯¹è¯æ•°æ®åŸºäºLLaMA2-7b-Chatè®­ç»ƒè€Œæ¥|[CareLlama2-7b-chat-sft-multi](https://huggingface.co/wangrongsheng/CareLlama2-7b-chat-sft-multi)|å‡ºè‰²çš„å¤šè½®å¯¹è¯èƒ½åŠ›|LLaMA2-7b-Chat|QLoRA|
|ç›‘ç£å¾®è°ƒ|ä¸°å¯Œé«˜æ•ˆåŒ»æ‚£å¯¹è¯æ•°æ®åŸºäºLLaMA2-7b-Chatè®­ç»ƒè€Œæ¥|[CareLlama2-7b-chat-sft-med](https://huggingface.co/wangrongsheng/CareLlama2-7b-chat-sft-med)|å‡ºè‰²çš„æ‚£è€…ç–¾ç—…è¯Šæ–­èƒ½åŠ›|LLaMA2-7b-Chat|QLoRA|
|ç›‘ç£å¾®è°ƒ|æ··åˆæ•°æ®åŸºäºLLaMA-7bè®­ç»ƒè€Œæ¥|[CareLlama1-7b-merge](https://huggingface.co/wangrongsheng/CareLlama1-7b-merge)|æ›´å‡ºè‰²çš„åŒ»ç–—å¯¹è¯èƒ½åŠ›|LLaMA-7b|LoRA|

> **Note**
>
> æ›´å¤šæ¨¡å‹æ­£åœ¨æŒç»­æ›´æ–°...

> *ä½¿ç”¨æ–¹æ³•*ï¼š
> 1. ä¸‹è½½ç›¸åº”çš„åº•åº§æ¨¡å‹ï¼›
> 2. å¦‚æœä¸ºLLaMAåˆ™[è½¬ä¸ºHFæ ¼å¼](https://github.com/WangRongsheng/CareLlama#1%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96)ï¼Œå¦‚æœä¸ºLLaMA-2ä¸”ä¸‹è½½çš„ä¸ºHFæ ¼å¼åˆ™ä¸éœ€è¦è½¬åŒ–ï¼›
> 3. ä¸‹è½½ä¸Šè¿°ä½ æƒ³è¦åŠ è½½çš„æƒé‡ï¼›
> 4. æ ¹æ®[æ¨ç†é…ç½®](https://github.com/WangRongsheng/CareLlama/tree/main#4%E6%8E%A8%E7%90%86%E9%85%8D%E7%BD%AE)å¼€å§‹ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ï¼›

# ğŸ“³ç»“æœæ¼”ç¤º

![](./assets/examples/demo4.png)

<details>
<summary>æŸ¥çœ‹æ›´å¤šæ¼”ç¤º</summary>

![](./assets/examples/demo1.png)
![](./assets/examples/demo2.png)
![](./assets/examples/demo3.png)
![](./assets/examples/demo5.png)
![](./assets/examples/demo6.png)

</details>

# ğŸ°å…è´£å£°æ˜

æœ¬é¡¹ç›®ç›¸å…³èµ„æºä»…ä¾›å­¦æœ¯ç ”ç©¶ä¹‹ç”¨ï¼Œä¸¥ç¦ç”¨äºå•†ä¸šç”¨é€”ã€‚ä½¿ç”¨æ¶‰åŠç¬¬ä¸‰æ–¹ä»£ç çš„éƒ¨åˆ†æ—¶ï¼Œè¯·ä¸¥æ ¼éµå¾ªç›¸åº”çš„å¼€æºåè®®ã€‚æ¨¡å‹ç”Ÿæˆçš„å†…å®¹å—æ¨¡å‹è®¡ç®—ã€éšæœºæ€§å’Œé‡åŒ–ç²¾åº¦æŸå¤±ç­‰å› ç´ å½±å“ï¼Œæœ¬é¡¹ç›®æ— æ³•å¯¹å…¶å‡†ç¡®æ€§ä½œå‡ºä¿è¯ã€‚å³ä½¿æœ¬é¡¹ç›®æ¨¡å‹è¾“å‡ºç¬¦åˆåŒ»å­¦äº‹å®ï¼Œä¹Ÿä¸èƒ½è¢«ç”¨ä½œå®é™…åŒ»å­¦è¯Šæ–­çš„ä¾æ®ã€‚å¯¹äºæ¨¡å‹è¾“å‡ºçš„ä»»ä½•å†…å®¹ï¼Œæœ¬é¡¹ç›®ä¸æ‰¿æ‹…ä»»ä½•æ³•å¾‹è´£ä»»ï¼Œäº¦ä¸å¯¹å› ä½¿ç”¨ç›¸å…³èµ„æºå’Œè¾“å‡ºç»“æœè€Œå¯èƒ½äº§ç”Ÿçš„ä»»ä½•æŸå¤±æ‰¿æ‹…è´£ä»»ã€‚

# ğŸ¥‚é¡¹ç›®å¼•ç”¨

å¦‚æœä½ ä½¿ç”¨äº†æœ¬é¡¹ç›®çš„æ¨¡å‹ï¼Œæ•°æ®æˆ–è€…ä»£ç ï¼Œè¯·å£°æ˜å¼•ç”¨ï¼š
```
@misc{wang2023carellama,
      title={CareLlama: Medical LLM, Open Source Driven for a Healthy Future}, 
      author={Rongsheng Wang, Tao Tan},
      year={2023},
      publisher = {GitHub},
      journal = {GitHub repository},
      howpublished = {\url{https://github.com/WangRongsheng/CareLlama}},
}
```

# ğŸ””ä½¿ç”¨è®¸å¯

æ­¤å­˜å‚¨åº“éµå¾ª[CC BY-NC-SA](https://creativecommons.org/licenses/by-nc-sa/4.0/) ï¼Œè¯·å‚é˜…è®¸å¯æ¡æ¬¾ã€‚

# ğŸ“šé¡¹ç›®å‚è€ƒ

#### åŒ»å­¦LLM
- https://github.com/llSourcell/DoctorGPT
- https://github.com/facebookresearch/llama-recipes
- https://github.com/Kent0n-Li/ChatDoctor
- https://github.com/hiyouga/LLaMA-Efficient-Tuning
- https://github.com/michael-wzhu/ShenNong-TCM-LLM
- https://github.com/michael-wzhu/ChatMed
- https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese
- https://github.com/SCIR-HI/Med-ChatGLM
- https://github.com/xionghonglin/DoctorGLM
- https://github.com/MediaBrain-SJTU/MING
- https://github.com/CMKRG/QiZhenGPT
- https://github.com/NLPxiaoxu/LLM-Pretrain-FineTune
- https://github.com/scutcyr/BianQue
- https://github.com/thomas-yanxin/Sunsimiao
- https://github.com/kbressem/medAlpaca
- https://github.com/FreedomIntelligence/HuatuoGPT
- https://github.com/shibing624/MedicalGPT
- https://github.com/chaoyi-wu/PMC-LLaMA
- https://github.com/pariskang/CMLM-ZhongJing
- https://github.com/SupritYoung/Zhongjing
- https://medical.chat-data.com/
- https://github.com/openmedlab/PULSE

#### éƒ¨ç½²LLM
- https://github.com/a16z-infra/llama2-chatbot
- https://github.com/liltom-eth/llama2-webui
- https://github.com/soulteary/docker-llama2-chat
- https://huggingface.co/spaces/LinkSoul/Chinese-Llama-2-7b
- https://github.com/mushan0x0/AI0x0.com
- https://github.com/Yidadaa/ChatGPT-Next-Web

#### LLMèµ„æº
- https://github.com/onejune2018/Awesome-Medical-Healthcare-Dataset-For-LLM
- https://github.com/WangRongsheng/MedQA-ChatGLM
- https://github.com/WangRongsheng/Use-LLMs-in-Colab
- https://github.com/HqWu-HITCS/Awesome-Chinese-LLM

![](./assets/images/end.png)
